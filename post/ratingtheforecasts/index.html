<!DOCTYPE html>
<html lang="en"><head>

  <meta name="generator" content="Hugo 0.74.3" />
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="author" content="Tom Saunders"><meta name="keywords" content="blog,tom,saunders,tomandthenews"><meta name="description" content=""><meta property="og:title" content="Rating the 2020 forecasts" />
<meta property="og:description" content="As of writing this, the election results in some states are still being tabulated. The four days after election night have seen the fortunes of both campaigns drastically change and with that the accuracy of models that universally confidently predicted a Biden win. On Tuesday night, when a Trump re-election seemed distinctly possible, many political commentators gave their, often unforgiving, opinions about the state of the forecaster/polling industry." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tomjs.org/post/ratingtheforecasts/" />
<meta property="article:published_time" content="2020-11-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-11-10T00:00:00+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Rating the 2020 forecasts"/>
<meta name="twitter:description" content="As of writing this, the election results in some states are still being tabulated. The four days after election night have seen the fortunes of both campaigns drastically change and with that the accuracy of models that universally confidently predicted a Biden win. On Tuesday night, when a Trump re-election seemed distinctly possible, many political commentators gave their, often unforgiving, opinions about the state of the forecaster/polling industry."/>

  <link rel="stylesheet" type="text/css" media="screen" href="https://tomjs.org/css/normalize.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="https://tomjs.org/css/main.css" />
  <link rel="stylesheet" type="text/css" media="screen" href="https://tomjs.org/css/all.css" />
<link rel="stylesheet" href="https://tomjs.org/css/katex.min.css" crossorigin="anonymous">
  <script defer src="https://tomjs.org/js/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://tomjs.org/js/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><link rel="stylesheet" type="text/css" media="screen" href="https://tomjs.org/css/css/custom.css" /><title>Rating the 2020 forecasts | @tomjs</title></head>
<body><header>

  <div id="avatar">
    <a href="https://tomjs.org/">
      <img src="/img/vitae.jpg" alt="@tomjs">
    </a>
  </div>

  <div id="titletext"><h2 id="title"><a href="https://tomjs.org/">@tomjs</a></h2></div>
  <div id="title-description"><p id="subtitle">More Words.</p><div id=social>
    <nav>
      <ul><li><a href="https://github.com/tomsaunders98"><i title="Github" class="icons fab fa-github"></i></a></li><li><a href="/index.xml"><i title="RSS" class="icons fas fa-rss"></i></a></li><li><a href="https://twitter.com/tomjs"><i title="Twitter" class="icons fab fa-twitter"></i></a></li><li><a href="https://www.linkedin.com/in/tomsaunders98/"><i title="LinkedIn" class="icons fab fa-linkedin-in"></i></a></li><li><a href="mailto:tomandthenews@gmail.com"><i title="Email" class="icons fas fa-envelope"></i></a></li><li><a href="https://tomjs.org/key.txt"><i title="PGP Key" class="icons fas fa-key"></i></a></li><li><a href="https://matrix.to/#/@tomandthenews:matrix.org"><i title="Matrix.org" class="icons fas fa-comments"></i></a></li></ul>
    </nav>
  </div>
  </div>
  <div id="mainmenu">
    <nav>
      <ul>
        
        <li><a href="/">Home</a></li>
        
        <li><a href="/about">About</a></li>
        
        <li><a href="/projects">My Projects</a></li>
        
        <li><a href="/post">All Posts</a></li>
        
      </ul>
    </nav>
  </div>
</header>
<main><div class="post">
<div class="author">

</div>
<div class="post-header">

<div class="meta">
<div class="date">
<span class="day">10</span>
<span class="rest">Nov 2020</span>
</div>
</div>

<div class="matter">
<h1 class="title">Rating the 2020 forecasts</h1>
</div>
</div>
<div class="markdown">

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>


<p>As of writing this, the election results in some states are still being tabulated. The four days after election night have seen the fortunes of both campaigns drastically change and with that the accuracy of models that universally confidently predicted a Biden win. On Tuesday night, when a Trump re-election seemed distinctly possible, many political commentators gave their, often unforgiving, opinions about the state of the forecaster/polling industry.</p>
<p>Now that most of the results in it seems appropriate to take a more in-depth look at the 2020 forecasts to see just how accurate they were.</p>
<div id="the-forecasts" class="section level2">
<h2>The forecasts</h2>
<table>
<thead>
<tr class="header">
<th>Forecast</th>
<th>Authors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://corymccartan.github.io/projects/president-20/">Cory McCartan</a></td>
<td>Cory McCartan</td>
</tr>
<tr class="even">
<td><a href="https://projects.fivethirtyeight.com/2020-election-forecast/">FiveThirtyEight</a></td>
<td><a href="https://twitter.com/natesilver538">Nate Silver</a></td>
</tr>
<tr class="odd">
<td><a href="https://jhkforecasts.com/">JHK Forecasts</a></td>
<td><a href="https://twitter.com/jhkersting">Jack Kersting</a></td>
</tr>
<tr class="even">
<td><a href="https://www.newstatesman.com/international/2020/11/us-2020-presidential-election-forecast-model-will-donald-trump-or-joe-biden">New Statesman</a></td>
<td><a href="https://twitter.com/BenNHWalker">Ben Walker</a></td>
</tr>
<tr class="odd">
<td><a href="https://www.predictit.org/">PredictIt</a></td>
<td>Victoria University of Wellington</td>
</tr>
<tr class="even">
<td><a href="https://election.princeton.edu/">Princeton Election Consortium</a></td>
<td><a href="https://twitter.com/SamWangPhD">Sam Wang</a></td>
</tr>
<tr class="odd">
<td><a href="https://projects.economist.com/us-2020-forecast/president">The Economist</a></td>
<td><a href="https://twitter.com/gelliottmorris">G. Elliot Morris</a>, <a href="https://twitter.com/StatModeling">Andrew Gelman</a> and <a href="https://twitter.com/MHeidemanns">Merlin Heidemanns</a></td>
</tr>
<tr class="even">
<td><a href="https://tomjs.org/Forecast2020/">tomjs</a></td>
<td><a href="https://twitter.com/tomjs/">Tom Saunders</a> (me)</td>
</tr>
</tbody>
</table>
</div>
<div id="the-states" class="section level2">
<h2>The states</h2>
<p>Each model that gave state level predictions in 2020 is represented on the chart below. The accuracy of each model’s prediction in each state was calculated using a <a href="https://en.wikipedia.org/wiki/Brier_score">brier score</a>. The score works by evaluating both whether a forecaster’s prediction was correct and how confident they were in that prediction. The more confident the forecaster is in an outcome which does not happen, the higher the brier score they are given. The chart also includes results from <a href="https://www.predictit.org/">PredictIt</a>, which is not an election model but a betting market that uses bets to crowdfund predictions for each state.</p>
<p><img src="/post/RatingTheForecasts_files/figure-html/modelbystates-1.png" width="1152" /></p>
<p>This chart shows what were the two biggest surprises of this election: Florida and North Carolina. The forecasters got it most wrong in Florida where the majority gave a high probability that the state would turn blue. Instead Trump was able to increase his margin by making inroads into the latino community. In second place and not far behind Florida was North Carolina. Most forecasters projected that Biden would win North Carolina , albeit with less certainty than in Florida, but in the end the state went narrowly for Trump. Every single modeler got these two states wrong.</p>
<p>There were less noticeable errors by forecasters in Iowa, Ohio, Arizona, Texas and Georgia where the outcomes were assumed by most forecasters to be far less certain with the majority of forecasters labeling at least some of these states as toss-ups. Ultimately, only Arizona and Georgia had close elections while the rest went firmly for Trump.</p>
<p>Most of the criticism of forecasters focused on these states, with the exception Arizona and Georgia, and this was only compounded by the fact that these states were some of the first to be called by the networks. Florida, where the most glaring errors were, was the first state to release voting figures.</p>
</div>
<div id="how-did-the-models-perform-overall" class="section level2">
<h2>How did the models perform overall ?</h2>
<p>The overall performance of each model was measured by two metrics. Firstly, a brier score weighted by electoral votes, this means that a prediction for Pennsylvania, which has 20 electoral votes, will count twice as much as a prediction for Missouri, which only has 10 electoral votes. Secondly, the <a href="https://en.wikipedia.org/wiki/Scoring_rule#Logarithmic_scoring_rule">logarithmic scoring rule</a>, also weighted by electoral votes. The log score is similar to the brier score but punishes misplaced confidence more severely than the brier score. The closer the log score is to zero, the more accurate the prediction.</p>
<p><img src="/post/RatingTheForecasts_files/figure-html/whowon-1.png" width="672" /></p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right">EV adjusted brier score</th>
<th align="right">EV adjusted log score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tomjs</td>
<td align="right">0.0783494</td>
<td align="right">-0.2231446</td>
</tr>
<tr class="even">
<td align="left">New Statesman</td>
<td align="right">0.0694494</td>
<td align="right">-0.2160690</td>
</tr>
<tr class="odd">
<td align="left">PredictIt</td>
<td align="right">0.0529454</td>
<td align="right">-0.2050114</td>
</tr>
<tr class="even">
<td align="left">FiveThirtyEight</td>
<td align="right">0.0659725</td>
<td align="right">-0.2014210</td>
</tr>
<tr class="odd">
<td align="left">The Economist</td>
<td align="right">0.0706210</td>
<td align="right">-0.2005267</td>
</tr>
<tr class="even">
<td align="left">Cory McCartan</td>
<td align="right">0.0657303</td>
<td align="right">-0.1998458</td>
</tr>
<tr class="odd">
<td align="left">Jhk Forecasts</td>
<td align="right">0.0656198</td>
<td align="right">-0.1899750</td>
</tr>
<tr class="even">
<td align="left">Princeton Election Consortium</td>
<td align="right">0.0618348</td>
<td align="right">-0.1850797</td>
</tr>
</tbody>
</table>
<p>Overall, the <a href="https://election.princeton.edu/">Princeton Election Consortium</a> was the most accurate model in the log score and second in the Brier Score. The betting market was surprisingly accurate, achieving the best brier score, but it did not do particularly well in the log score due to consistently under confident predictions across many states (such as 5% chance of Trump winning New York or 10% chance of Biden winning Alaska).</p>
<p>Rather than having any unique predictive abilities, it seems the betting markets gain most of their success by consistently hedging their predictions even when it makes little sense. This makes their predictions less likely to be completely wrong in a particular state, but generally less accurate over all 50 states.</p>
<p>Special mention also goes out to <a href="https://jhkforecasts.com/">JHK Forecasts</a>, built by a student at the University of Alabama, which came second in both the weighted log score and the weighted brier score. Unsurprisingly, my model fared pretty badly but that is what you get when you don’t include a correlated error component or national polls. This was my first attempt at modeling and was mainly about learning the ropes, you can read about it <a href="https://tomjs.org/post/electionforecast/">here</a></p>
<p><img src="/post/RatingTheForecasts_files/figure-html/modelovertime-1.png" width="672" /></p>
</div>
<div id="how-do-the-models-perform-compare-to-2016" class="section level2">
<h2>How do the models perform compare to 2016?</h2>
<p>Early on election night it seemed that forecasts might be as wrong about the 2020 election as they were in 2016. However, as the votes were counted over the next few days it turns out that the forecasts this year were substantially more accurate than in 2016.</p>
<pre><code>## Warning: Removed 1580 rows containing missing values (geom_text).</code></pre>
<p><img src="/post/RatingTheForecasts_files/figure-html/v2016-1.png" width="672" /></p>
<p>As election day neared the gap in accuracy between the models in 2016 and 2020 widened considerably. The only models from 2016 that approached 2020 levels of accuracy were 538’s “<a href="https://projects.fivethirtyeight.com/2016-election-forecast/#plus">Polls-plus</a>”and “<a href="https://projects.fivethirtyeight.com/2016-election-forecast/">Polls-only</a>” models which did much better than any other 2016 model.</p>
</div>
<div id="who-called-the-most-states" class="section level2">
<h2>Who called the most states?</h2>
<p>This article has primarily used measures that score probabilistic predictions but not all forecasts have a probabilistic component. There are other forecasts that only predict the outcome in each state. To include these there is a simpler metric: the number of states that each model called correctly.</p>
<table>
<thead>
<tr class="header">
<th align="left">Model</th>
<th align="right">No. states called correctly</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Crystal Ball</td>
<td align="right">49</td>
</tr>
<tr class="even">
<td align="left">Cory McCartan</td>
<td align="right">48</td>
</tr>
<tr class="odd">
<td align="left">FiveThirtyEight</td>
<td align="right">48</td>
</tr>
<tr class="even">
<td align="left">Jhk Forecasts</td>
<td align="right">48</td>
</tr>
<tr class="odd">
<td align="left">New Statesman</td>
<td align="right">48</td>
</tr>
<tr class="even">
<td align="left">PredictIt</td>
<td align="right">48</td>
</tr>
<tr class="odd">
<td align="left">Princeton Election Consortium</td>
<td align="right">48</td>
</tr>
<tr class="even">
<td align="left">The Economist</td>
<td align="right">48</td>
</tr>
<tr class="odd">
<td align="left">Poll Average</td>
<td align="right">48</td>
</tr>
<tr class="even">
<td align="left">tomjs</td>
<td align="right">47</td>
</tr>
</tbody>
</table>
<p>Overall there was substantial conformity in state-level forecasts, apart from mine where I mistakenly predicted Ohio would turn blue and <a href="https://www.270towin.com/maps/sabatos-crystal-ball-2020-president">Larry Sabato’s Crystall ball</a> which was the only model to correctly predict that Florida would remain republican. .</p>
</div>
<div id="what-about-the-polls" class="section level2">
<h2>What about the polls ?</h2>
<p>Some forecasts also project the vote share of each party on election day in each state. To evaluate the accuracy of these vote share predictions we calculated the <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">root-mean-square error</a> (rmse) of the forecaster’s projected vote margins.</p>
<p>The rmse of each forecaster are calculated against the two-party vote share, as not all states included 3rd parties in their projections. Similarly to the other metrics, a smaller score corresponds to a more accurate projection with a score of 0 meaning a perfect prediction. The poll averages refer to the average of the polls in each state on election day.</p>
<p><img src="/post/RatingTheForecasts_files/figure-html/rmse-1.png" width="672" /></p>
<p>As the numbers stand at the moment, it’s clear that the polls were just as wrong, if not more, than they were in 2016. After 2016, many poll organisations started to weigh by education-level in their surveys, in order to account for what they believed were overlooked Trump voters. However, preliminary poll analysis suggests that polls were least accurate in states with large numbers of white non-college educated voters. Interestingly, one of the few pollsters to correctly call the race in Iowa, a state where most pollsters were drastically wrong, was Ann Selzer’s Iowa poll from October 26-29 and Ann Selzer does not weigh for education in her samples.</p>
<p>It’s clear that 2020 turned out to be actually quite a good election for most forecasts, but it’s equally as clear that there is still a lot of work to be done by pollsters to better understand the shifting demographic make-up of the United States.</p>
<p>The code for this article is available <a href="https://gist.github.com/tomsaunders98/866c1cf791c5dae842db89bcd01d6542">here</a> and you can download the data <a href="https://raw.githubusercontent.com/tomsaunders98/tomsaunders98.github.io/master/forecastdata.zip">here</a>.</p>
</div>

</div>
<div class="tags">











</div></div>

</main><footer>



</footer>
</body>
</html>
